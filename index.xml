<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>He Jinhao</title>
    <link>http://david-willo.github.io//</link>
      <atom:link href="http://david-willo.github.io//index.xml" rel="self" type="application/rss+xml" />
    <description>He Jinhao</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 20 Jul 2020 21:44:19 +0800</lastBuildDate>
    <image>
      <url>http://david-willo.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>He Jinhao</title>
      <link>http://david-willo.github.io//</link>
    </image>
    
    <item>
      <title>Ground and Aerial Collaborative Mapping in Urban Environments</title>
      <link>http://david-willo.github.io//project/ground_n_aerial_collaborative_mapping_in_urban_environment/</link>
      <pubDate>Mon, 20 Jul 2020 21:44:19 +0800</pubDate>
      <guid>http://david-willo.github.io//project/ground_n_aerial_collaborative_mapping_in_urban_environment/</guid>
      <description>&lt;p&gt;A system that uses heterogeneous platforms(cars and drones) mounted with 16-beam LiDARs and RGB cameras to build a 3D point cloud map of the environment.&lt;/p&gt;





  
  











&lt;figure &gt;



  &lt;img data-src=&#34;http://david-willo.github.io//project/ground_n_aerial_collaborative_mapping_in_urban_environment/GA_visual_hud2c94d2910778b0690c945527e63046b_787660_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1127&#34; height=&#34;494&#34;&gt;




&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Structure-awared Visual-LiDAR Odometry and Dense Mapping</title>
      <link>http://david-willo.github.io//project/structure_awared_vlo_-n_dense_mapping/</link>
      <pubDate>Mon, 20 Jul 2020 21:25:19 +0800</pubDate>
      <guid>http://david-willo.github.io//project/structure_awared_vlo_-n_dense_mapping/</guid>
      <description>&lt;p&gt;A Visual-LiDAR SLAM system considering both point, line and planar constraints. This system employ multi-sensor fusion and dense mapping in a same pipeline, which is robust under degenerated scenarios.





  
  











&lt;figure &gt;



  &lt;img data-src=&#34;http://david-willo.github.io//project/structure_awared_vlo_-n_dense_mapping/result_hu79e644edaeb3d5ef42a5c92b8205ccc3_623683_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1058&#34; height=&#34;665&#34;&gt;




&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Map fusion method suitable for sub-map with few overlapping parts</title>
      <link>http://david-willo.github.io//patent/map-fusion-method/</link>
      <pubDate>Sun, 29 Sep 2019 00:00:00 +0000</pubDate>
      <guid>http://david-willo.github.io//patent/map-fusion-method/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Synthetic Data Generation for Multi-Robot SLAM</title>
      <link>http://david-willo.github.io//project/synthetic_data_generation_for_multi_robot_slam/</link>
      <pubDate>Sat, 20 Jul 2019 20:43:10 +0800</pubDate>
      <guid>http://david-willo.github.io//project/synthetic_data_generation_for_multi_robot_slam/</guid>
      <description>




  
  











&lt;figure &gt;



  &lt;img data-src=&#34;http://david-willo.github.io//project/synthetic_data_generation_for_multi_robot_slam/paths_hu9bab6ba8d4a199355ff8d024133834fc_582247_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1269&#34; height=&#34;699&#34;&gt;




&lt;/figure&gt;

&lt;p&gt;Time-synchronized simulation sensor data including LiDAR point cloud, stereo RGB images as well as IMU(Inertial Measurement Unit) data from multiple robots are recorded into ROS bags, which is sufficient for simulation tests for multi-robot SLAM systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LiDAR-based Collaborative Localization and Mapping</title>
      <link>http://david-willo.github.io//project/lidar_based_collaborative_localization_and_mapping/</link>
      <pubDate>Thu, 20 Jun 2019 20:10:54 +0800</pubDate>
      <guid>http://david-willo.github.io//project/lidar_based_collaborative_localization_and_mapping/</guid>
      <description>&lt;p&gt;A system that uses multiple robots mounted with 16-beam LiDARs to build a 3D point cloud map of the environment.





  
  











&lt;figure &gt;



  &lt;img data-src=&#34;http://david-willo.github.io//project/lidar_based_collaborative_localization_and_mapping/3ugv_hu94488767005b0dbe1b2db57bed19d0c3_277003_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1119&#34; height=&#34;856&#34;&gt;




&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-unmanned-platform synchronous positioning and map construction method under limitation of communication bandwidth and distance</title>
      <link>http://david-willo.github.io//patent/multi-unmanned-platform-synchronous-positioning-and-map-construction/</link>
      <pubDate>Fri, 15 Mar 2019 00:00:00 +0000</pubDate>
      <guid>http://david-willo.github.io//patent/multi-unmanned-platform-synchronous-positioning-and-map-construction/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Embedding Temporally Consistent Depth Recovery for Real-time Dense Mapping in Visual-inertial Odometry</title>
      <link>http://david-willo.github.io//project/cheng-embedding-2018/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>http://david-willo.github.io//project/cheng-embedding-2018/</guid>
      <description>&lt;p&gt;A method to embed depth recovery algorithm into VIO-based sparse visual SLAM system for real-time dense mapping.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A subspace-based stabilization scheme to maintain the temporal consistency of visual landmarks.&lt;/li&gt;
&lt;li&gt;A hierarchical pipeline for edge-preserving depth interpolation.&lt;/li&gt;
&lt;li&gt;The whole pipeline is implemented using CPU only.&lt;/li&gt;
&lt;/ul&gt;





  
  











&lt;figure &gt;



  &lt;img data-src=&#34;http://david-willo.github.io//project/cheng-embedding-2018/album/3d_map_1_hu7371dfd10b02947e294cd0190ff80fdd_377335_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1445&#34; height=&#34;621&#34;&gt;




&lt;/figure&gt;

&lt;!-- 







  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-album&#34; href=&#34;http://david-willo.github.io//project/cheng-embedding-2018/album/3d_map_1.png&#34; &gt;
  &lt;img data-src=&#34;http://david-willo.github.io//project/cheng-embedding-2018/album/3d_map_1_hu7371dfd10b02947e294cd0190ff80fdd_377335_0x190_resize_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;442&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-album&#34; href=&#34;http://david-willo.github.io//project/cheng-embedding-2018/album/flowchart.png&#34; &gt;
  &lt;img data-src=&#34;http://david-willo.github.io//project/cheng-embedding-2018/album/flowchart_hu6fd5de7c787896b97c8b103cfa7809a8_46319_0x190_resize_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;694&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-album&#34; href=&#34;http://david-willo.github.io//project/cheng-embedding-2018/album/landmarkmatch.png&#34; &gt;
  &lt;img data-src=&#34;http://david-willo.github.io//project/cheng-embedding-2018/album/landmarkmatch_hu204e5f7f43ac0ea1bfa14169ff8d5fbd_175418_0x190_resize_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-album&#34; href=&#34;http://david-willo.github.io//project/cheng-embedding-2018/album/map_grey.png&#34; &gt;
  &lt;img data-src=&#34;http://david-willo.github.io//project/cheng-embedding-2018/album/map_grey_huae2426d83be36ac398bee316d6fa9fd4_374747_0x190_resize_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;802&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-album&#34; href=&#34;http://david-willo.github.io//project/cheng-embedding-2018/album/result.png&#34; &gt;
  &lt;img data-src=&#34;http://david-willo.github.io//project/cheng-embedding-2018/album/result_hu9aa0b8ef978779e84cb6a9ba822303f7_35347_0x190_resize_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  

  
&lt;/div&gt; --&gt;
&lt;!-- 











  


&lt;video controls &gt;
  &lt;source src=&#34;IROS18_1068_VI_fi.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt; --&gt;
</description>
    </item>
    
    <item>
      <title>Embedding Temporally Consistent Depth Recovery for Real-time Dense Mapping in Visual-inertial Odometry</title>
      <link>http://david-willo.github.io//publication/cheng-embedding-2018/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>http://david-willo.github.io//publication/cheng-embedding-2018/</guid>
      <description>&lt;!-- 







  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-album&#34; href=&#34;http://david-willo.github.io//publication/cheng-embedding-2018/album/3d_map_1.png&#34; &gt;
  &lt;img data-src=&#34;http://david-willo.github.io//publication/cheng-embedding-2018/album/3d_map_1_hu7371dfd10b02947e294cd0190ff80fdd_377335_0x190_resize_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;442&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-album&#34; href=&#34;http://david-willo.github.io//publication/cheng-embedding-2018/album/flowchart.png&#34; &gt;
  &lt;img data-src=&#34;http://david-willo.github.io//publication/cheng-embedding-2018/album/flowchart_hu6fd5de7c787896b97c8b103cfa7809a8_46319_0x190_resize_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;694&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-album&#34; href=&#34;http://david-willo.github.io//publication/cheng-embedding-2018/album/landmarkmatch.png&#34; &gt;
  &lt;img data-src=&#34;http://david-willo.github.io//publication/cheng-embedding-2018/album/landmarkmatch_hu204e5f7f43ac0ea1bfa14169ff8d5fbd_175418_0x190_resize_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-album&#34; href=&#34;http://david-willo.github.io//publication/cheng-embedding-2018/album/map_grey.png&#34; &gt;
  &lt;img data-src=&#34;http://david-willo.github.io//publication/cheng-embedding-2018/album/map_grey_huae2426d83be36ac398bee316d6fa9fd4_374747_0x190_resize_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;802&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
  &lt;a data-fancybox=&#34;gallery-album&#34; href=&#34;http://david-willo.github.io//publication/cheng-embedding-2018/album/result.png&#34; &gt;
  &lt;img data-src=&#34;http://david-willo.github.io//publication/cheng-embedding-2018/album/result_hu9aa0b8ef978779e84cb6a9ba822303f7_35347_0x190_resize_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
  &lt;/a&gt;
  

  
&lt;/div&gt; --&gt;
&lt;!-- 











  


&lt;video controls &gt;
  &lt;source src=&#34;IROS18_1068_VI_fi.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt; --&gt;
</description>
    </item>
    
    <item>
      <title>An Indoor Positioning System On Smart Phones</title>
      <link>http://david-willo.github.io//project/an-in-door-positioning-system-on-smart-phones/</link>
      <pubDate>Wed, 10 Jan 2018 15:16:22 +0800</pubDate>
      <guid>http://david-willo.github.io//project/an-in-door-positioning-system-on-smart-phones/</guid>
      <description>&lt;p&gt;A system to help users to localize themselves in a shopping center using a smart phone.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image frames and gyroscope data are captured and upload to the server using an Android app.&lt;/li&gt;
&lt;li&gt;Detect and track shop signs on image sequence.&lt;/li&gt;
&lt;li&gt;Compute the positioning result and send it back to the user front-end.





  
  











&lt;figure &gt;



  &lt;img data-src=&#34;http://david-willo.github.io//project/an-in-door-positioning-system-on-smart-phones/sign_detect_huca98343d74a109ad266947bef9539e2f_333447_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1012&#34; height=&#34;397&#34;&gt;




&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Quick depth restoring method for three-dimensional reconstruction</title>
      <link>http://david-willo.github.io//patent/quick-depth-restoring/</link>
      <pubDate>Fri, 22 Dec 2017 00:00:00 +0000</pubDate>
      <guid>http://david-willo.github.io//patent/quick-depth-restoring/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust Visual Inertial Odometry With Consistent Sparsification</title>
      <link>http://david-willo.github.io//project/robust_visual_inertial_odometry_with_consistent_sparsification/</link>
      <pubDate>Wed, 20 Sep 2017 19:43:37 +0800</pubDate>
      <guid>http://david-willo.github.io//project/robust_visual_inertial_odometry_with_consistent_sparsification/</guid>
      <description>&lt;p&gt;A hybrid solution integrating the features of adaptive sensor fusion, robust loss
function, and consistent sparsification into a visual-inertial odometry (VIO) system, improving the system&amp;rsquo;s accuracy, robustness, and efficiency respectively.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Switchable Constraints and Dynamic Covariance Scaling loss is use in visual residual to improve the robustness of the VIO system&lt;/li&gt;
&lt;li&gt;The weights of inertial residuals are adjusted adaptively according to the motion states of the system&lt;/li&gt;
&lt;li&gt;the marginalized information matrix is consistently sparsified to reduce the fill-in effect&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
